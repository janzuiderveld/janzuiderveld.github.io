<!DOCTYPE html>
<html>
<head>
    <title>Jan Zuiderveld</title>

    <!-- Meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom Styles -->
    <style>
          body {
            font-family: 'sans-serif';
            font-size: 16px;
            background-color: #FFFFFF;
            color: #4F6071;
          }
          #header {
            background-color: #f4f4f4;
            /*background-color: #FFFFFF;*/
            display: flex;
            align-items: flex-end;
            padding-top:60px;
            padding-bottom:60px;
          }
          #footer {
            background-color: #FFFFFF;
            padding:60px;
          }
          #portrait {
            border: 3px solid white;
          }
          #header-text {
            margin-top: 60px;
            margin-left: 220px;
          }
          #header-text-name {
            font-size: 40px;
          }
          #header-text-email {
            font-size: 20px;
            font-style: italic;
          }
          .header-text-desc {
            font-size: 20px;
          }
          .vspace-top {
            margin-top: 30px;
          }
          .vspace-top-news {
              margin-top: 15px;
          }
          .paper-image {
            width: 150px;
          }
          .news-date {
              font-weight: bold;
          }
          .paper-title {
            font-weight: bold;
          }
          .paper-authors {
            font-style: italic;
          }
    </style>
</head>

<body>
    <div id='header'>
        <div class='container'>
            <div class='row'>
                <!-- <div class="col-sm-3 offset-sm-1">
                    <img src='imgs/portrait.jpeg' class='img-fluid' id='portrait'>
                </div> -->

                <div class="col">
                  <div id='header-text-name'>
                      Jan Zuiderveld
                  </div>
                  <div id='header-text-email'>
                    jan@warana.xyz
                  </div>
                  <div>
                    <a href="https://github.com/janzuiderveld">[GitHub]</a>
                    <a href="https://scholar.google.com/citations?user=mTZpgswAAAAJ&hl=en">[Google Scholar]</a>
                    <a href="https://www.linkedin.com/in/jan-zuiderveld/">[LinkedIn]</a>
                    <a href="https://www.instagram.com/_warana/">[Instagram]</a>
<!-- 		    <a href="https://ko-fi.com/warana">[Ko-fi (Tooling + Support)]</a> -->
                    <a href="docs/jzuiderveld_CV.pdf">[CV]</a>
                  </div>

                </div>
            </div>
        </div>
    </div>


    <div class='container'>
        <div class='row vspace-top'>
            <div class='col offset-sm-1'>
                <h1>
                    About me
                </h1>

                <p>
                    I'm a graduate of the MSc Artificial Intelligence (University of Amsterdam). Currently I'm researching applications of machine learning in art at the Royal Conservatoire and the Royal Academy of Art in The Hague. 
			<br>
			<br>
		    Motivated by curiosity and fascination for the fast developing field of artificial intelligence, I try to understand and shed light on the profound processes underlying AI methods that are transforming our world by approaching the topic from both a creative- and scientific perspective. I aim to raise awareness of the power of machine learning as a creativity enhancing tool and the possibilities it creates for anyone, even those that do not feel creative, to create.
 		    


                </p>


                <div class='vspace-top'>
                    <h1>News</h1>
                </div>

                <!--- List of news ---!>

		<div class='row vspace-top-news'>
                        <div class="col-sm-2 news-date">
                            April 2023
                    </div>
                    <div class="col">
                    I'm thrilled to announce that my latest project, "Touching Distance," has been funded by the Digital Culture Grant Scheme of the Creative Industries Fund. "Touching Distance" is an immersive installation that uses cutting-edge technology to transform visitors' moving bodies into low-latency, high precision, theremin-like sensors. As visitors interact with the space, they create visually and acoustically rich representations of their movements, fostering a sense of intimacy and connection with the environment.

                    <br>
                    <br>

                    The project aims to push the boundaries of human interaction with technology, exploring how art, science, and digital culture intersect. This unique experience enables participants to engage in a dynamic, multi-sensory encounter that reflects the fluidity and complexity of human connection. Stay tuned for progress updates and exhibition details!
                        <br>
                        <br>

                    </div>
                </div>

		<div class='row vspace-top-news'>
                        <div class="col-sm-2 news-date">
                            January 2023
                    </div>
                    <div class="col">
			I gave a seminar at Whello, a leading digital marketing agency, on the use of generative AI in business. I covered the latest advancements in the field and shared my insights on how businesses can leverage these technologies for growth and innovation. The seminar was well-received and sparked interesting discussions. I am now open for inquiries and available for consultation on how to make the best use of generative AI!
                        <br>
                        <br>

                    </div>
                </div>


		<div class='row vspace-top-news'>
                        <div class="col-sm-2 news-date">
                            October 2022
                    </div>
                    <div class="col">
			Exciting news! I gave my installation "MIDIalogue" a large upgrade. The new version features upgraded sensors and improved machine learning architecture, providing a more responsive and accurate musical experience for visitors. The virtual harp is ready to engage in musical conversations like never before. Bookings are now open for festivals, exhibitions, club nights and corporate events. Don't miss the opportunity to experience the latest advancements in musical interaction with technology. Contact me if you're interested!
                        <br>
                        <br>

                    </div>
                </div>

                    <div class='row vspace-top-news'>
                        <div class="col-sm-2 news-date">
                            June 2022
                    </div>
                    <div class="col">
                        I published my work <a href="https://arxiv.org/pdf/2206.01661.pdf">Style-Content Disentanglement in Language-Image Pretraining Representations for Zero-Shot Sketch-to-Image Synthesis</a> on Arxiv. I found that CLIP embeddings exhibit a similar kind of compositionality as seen in word2vec embeddings, e.g, king - man + woman = queen, a hunch I had based on their sharing of contrastive training. 
                        <br>
			<br>
                        My results demonstrate that this can be exploited to achieve an open-domain sketch-to-image model competitive with state-of-the-art instance-level models, while only depending on pretrained off-the-shelf models and a fraction of the data.
                        
                        <br>
                        <br>

                    </div>
                </div>

                    <div class='row vspace-top-news'>
                        <div class="col-sm-2 news-date">
                            May 2022
                    </div>
                    <div class="col">
                        This month a new installation I made dubbed "Dream Machine" will be exhibited in <a href="https://www.deschoolamsterdam.nl/en/">De School</a> next to MIDIalogue. I'll soon publish a paper about the machine learning techniques I've used for it's sketch-to-image algorithm.
                                                
                        <br>
			<br>
                        The installation consists of a hacked 00's Xerox photocopier. In front of the Xerox machine is a drawing table with paper and markers, inviting visitors to get creative. The machine scans input drawings, generates an artistic image based on the content of the input, and prints the output.
                        
                        <br>
                        <br>

                    </div>
                </div>


                    <div class='row vspace-top-news'>
                        <div class="col-sm-2 news-date">
                            December 2021
                    </div>
                    <div class="col">
                        My paper <a href="https://arxiv.org/abs/2111.08462">Towards Lightweight Controllable Audio Synthesis using Conditional Implicit Neural Representations</a> based on my work at AMLab is accepted at both <a href="https://neuripscreativityworkshop.github.io/2021/">Machine Learning for Creativity and Design</a> and <a href="https://neurips.cc/Conferences/2021/ScheduleMultitrack?event=21878">Deep Generative Models and Downstream Applications</a> at NeurIPS '21. I'm invited to give an oral presentation at the latter. 
                        
                        <br>
                        <br>


                        My work "MIDIalogue" is selected to be featured in the <a href="https://neuripscreativityworkshop.github.io/2021/#/gallery">Machine Learning for Creativity and Design</a> gallery.
                    </div>
                </div>
                    
                <br>
                    <div class='row vspace-top-news'>
                        <div class="col-sm-2 news-date">
                            October 2021
                    </div>
                    <div class="col">
                        My first installation, "MIDIalogue", will be exhibited at <a href="https://www.deschoolamsterdam.nl/en/">De School</a> for the coming editions of the "a Perception of Space and Time" series of events.
                        
                        <br>
			<br>

                        MIDIalogue functions as an interface between visitors and a transformer neural network trained for melody generation and continuation. The interface consists of immaterial, but visible and pluckable strings. Like a virtual harp. The machine listens to visitors' melodic input, produces musical reactions and imposes possible conversation directions accordingly, assuring a harmonious, ever-evolving 
                        discourse. 

                    </div>
                </div>

                <br>
                    <div class='row vspace-top-news'>
                        <div class="col-sm-2 news-date">
                            September 2021
                    </div>
                    <div class="col">
                       I graduated cum laude from the MSc Artificial Intelligence at the University of Amsterdam, with my thesis on implicit neural representations applied to Audio: <a href="docs/Jan_Thesis_Msc.pdf"> Representing Audio in a Distribution of Continuous Functions</a>.
                       <br>
                       <br>

                       Also, I'm starting a new master: Art Science at the Royal Conservatoire and the Royal Academy of Art in The Hague. During my time here I'll be researching applications of machine learning in art.
                    </div>
                </div>
                <br>
                    <div class='row vspace-top-news'>
                        <div class="col-sm-2 news-date">
                            January 2021
                    </div>
                    <div class="col">
                        I just started a research internship at the Amsterdam Machine Learning Lab (<a href="https://amlab.science.uva.nl/">AMLab</a>). I'll research applications of implicit neural representations in generative networks for audio synthesis, supervised by drs. Marco Federici and dr. Erik Bekkers.
                    </div>
                </div>
                


                <div class='vspace-top'>
                    <h1>Publications</h1>
                </div>

                <!--- List of publications ---!>
                
                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src='imgs/audio_PCINRs_thumb.png' class='img-fluid'>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
			    Towards Lightweight Controllable Audio Synthesis with Conditional Implicit Neural Representations
                        </div>
                        <div class='paper-desc'>
				NeurIPS 2021, Deep Generative Models and Downstream Applications (Oral) <br>
				NeurIPS 2021, Machine Learning for Creativity and Design (Poster) 
                        </div>
                        <div class='paper-authors'>
                            Jan Zuiderveld, Marco Federici, Erik J. Bekkers
                        </div>
                        <div>
                            <a href="https://arxiv.org/abs/2111.08462">[Paper]</a>
                            <a href="https://janzuiderveld.github.io/audio-PCINRs/">[Project page]</a>
                            <a href="https://github.com/janzuiderveld/continuous-audio-representations">[Code]</a>

                            <br>
                            <br>
                            In this work, we aim to shed light on the potential of Conditional Implicit Neural Representations (CINRs) as lightweight backbones in generative frameworks for audio synthesis. Our experiments show that small Periodic Conditional INRs (PCINRs) learn faster and generally produce quantitatively better audio reconstructions than Transposed Convolutional Neural Networks with equal parameter counts. However, their performance is very sensitive to activation scaling hyperparameters. We validate noise presence can be minimized by applying standard weight regularization during training or decreasing the compositional depth of PCINRs, and suggest directions for future research.


                        </div>
                    </div>
                </div>
               


                <div class='row vspace-top'>
                    <div class="col-sm-3">
                        <img src='imgs/s2i_thum.png' class='img-fluid'>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
			    Style-Content Disentanglement in Language-Image Pretraining Representations for Zero-Shot Sketch-to-Image Synthesis
                        </div>
                        <div class='paper-desc'>
                    Arxiv
                        </div>
                        <div class='paper-authors'>
                            Jan Zuiderveld
                        </div>
                        <div>
                            <a href="https://arxiv.org/abs/2206.01661">[Paper]</a>
                            <!-- <a href="https://janzuiderveld.github.io/audio-PCINRs/">[Project page]</a> -->
                            <a href="https://github.com/janzuiderveld/sketch2image">[Code]</a>
                        
                            <br>
                            <br>

                            In this work, we propose and validate a framework to leverage language-image pretraining representations for training-free zero-shot sketch-to-image synthesis. Our approach for disentangling style and content entails a simple method consisting of elementary arithmetic assuming compositionality of information in representations of input sketches. Our results demonstrate that this approach is competitive with state-of-the-art instance-level open-domain sketch-to-image models, while only depending on pretrained off-the-shelf models and a fraction of the data.
                        
                        </div>
                    </div>
                </div>





                <div class='vspace-top'>
                    <h1>Works</h1>

                    <div class='row vspace-top'>
                        <div class="col-sm-3">
                            <img src='imgs/cut_3.jpg' class='img-fluid'>
                        </div>
    
                        <div class="col">
                            <div class='paper-title'>
                    MIDIalogue
                            </div>
                            <div class='paper-desc'>
                        2021 - Traumburg Festival (Dornburg, DE)
                        <br>
			2021 - Machine Learning for Creativity and Design, NeurIPS (Vancouver, CA)
		        <br>
                        2021/2022 - A Perception of Space and Time, De School (Amsterdam, NL)
                        <br>
                        2022 - Amsterdam Dance Event, Garage Noord (Amsterdam, NL)
			<br>
                        2022 - Markt Centraal (Amsterdam, NL)
			<br>
                        2022 - B.I.O.D.I.V.E.R.S., Ruigoord (Amsterdam, NL)
				    
                        
                            </div>
                            <!-- <div class='paper-authors'>
                                Jan Zuiderveld
                            </div> -->
                            <div>
                                <a href="https://drive.google.com/drive/folders/1slMSdRrkUQ2sokIXaIfQyUFC2BrqWzUY">[Documentation]</a>
                            
                                <br>
                                <br>
    

                                Attempting to create a spirited dialogue between man and machine, MIDIalogue functions as an interface between visitors and a transformer neural network trained for melody generation and continuation. The interface consists of immaterial, but visible and pluckable strings. Like a virtual harp. The machine listens to visitors' melodic input, produces musical reactions and imposes possible conversation directions accordingly, assuring a harmonious, ever-evolving 
                                discourse. 
                                <br>

                                The physical installation consists of 7 individually moveable lasers combined with freely placeable mirrors to create site-specific patterns. Mounted next to every laser is a lidar sensor for measuring long- and fine-grained distance.

                                    
    
                    
                            
                            </div>
                        </div>
                    </div>

                </div>
             
             
                <div class='row vspace-top'>
                        <div class="col-sm-3">
                            <img src='imgs/cut_5.jpg' class='img-fluid'>
                        </div>
    
                        <div class="col">
                            <div class='paper-title'>
                    Dream Machine
                            </div>
                            <div class='paper-desc'>
                        2022 - A Perception of Space and Time, De School (Amsterdam, NL)
                        <br>
                        2022 - Royal Academy of Art (The Hague, NL)
		    	<br>
			2022 - BODEGA, ISO Amsterdam (Amsterdam, NL)	    
			<br>
                        2023 - De Nieuw, De School (Amsterdam, NL)
			
                        
                            </div>
                            <!-- <div class='paper-authors'>
                                Jan Zuiderveld
                            </div> -->
                            <div>
                                <a href="https://drive.google.com/drive/folders/19VXNeb9zCwF1-96Wm3_Vp34KcQaZk0vE">[Documentation]</a>
                            
                                <br>
                                <br>
    
                                This installation consists of a hacked 00's Xerox photocopier. In front of the Xerox machine is a drawing table with paper and markers, inviting visitors to get creative. To nudge visitors to use the machine, the floor around the machine is filled with drawings and outputs. The machine has a big red button. When pressed, the machine will scan an input drawing, generate an artistic image (painting, 3D render, sculpture, woodcut, etc.) based on the content of the input, and print out the output. Visitors can take their printed AI collaborated artwork home. 
                                
                                <br>
    
                                The idea behind this installation is to create a space where people can interact with a machine that is itself creative, raising awareness of the potential of machine learning as a tool for creativity. The machine is designed to inspire playfulness and be a fun and easy way for people to create art, without needing any prior experience or knowledge.
                            
                            </div>
                        </div>
                    </div>

                </div>

            

        </div>
    </div>


    <div id='footer' class='vspace-top'>
    <div>

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="js/bootstrap.min.js"></script>
</body>

</html>
