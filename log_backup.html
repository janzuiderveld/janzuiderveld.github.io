
                
                                <div class='vspace-top'>
                                    <h1>Log</h1>
                                </div>
                
                                <!--- List of news ---!>
                
                        <div class='row vspace-top-news'>
                                        <div class="col-sm-2 news-date">
                                            April 2023
                                    </div>
                                    <div class="col">
                            I'm thrilled to announce that my latest project, "Touching Distance" is supported by the Digital Culture Grant of the Creative Industries Fund NL and De School. "Touching Distance" is an immersive installation that transforms visitors' moving bodies into low-latency, high precision, theremin-like sensors. As visitors interact with each other, their body-to-body distance and the amount of skin-to-skin touch are used to create visually and acoustically rich representations of their closeness. By focusing on inducing intimacy through curiosity, "Touching Distance" enables participants to engage in a dynamic, multi-sensory encounter that reflects the fluidity and complexity of human connection. 
                            <br>
                                        <br>
                                    </div>
                                </div>
                
                        <div class='row vspace-top-news'>
                                        <div class="col-sm-2 news-date">
                                            January 2023
                                    </div>
                                    <div class="col">
                            I gave a seminar at Whello, a leading digital marketing agency, on the use of generative AI in business. I covered the latest advancements in the field and shared my insights on how businesses can leverage these technologies for growth and innovation. The seminar was well-received and sparked interesting discussions. I am now open for inquiries and available for consultation on how to make the best use of generative AI!
                                        <br>
                                        <br>
                
                                    </div>
                                </div>
                
                
                        <div class='row vspace-top-news'>
                                        <div class="col-sm-2 news-date">
                                            October 2022
                                    </div>
                                    <div class="col">
                            Exciting news! I gave my installation "MIDIalogue" a large upgrade. The new version features upgraded sensors and improved machine learning architecture, providing a more responsive and accurate musical experience for visitors. The virtual harp is ready to engage in musical conversations like never before. Bookings are now open for festivals, exhibitions, club nights and corporate events. Don't miss the opportunity to experience the latest advancements in musical interaction with technology. Contact me if you're interested!
                                        <br>
                                        <br>
                
                                    </div>
                                </div>
                
                                    <div class='row vspace-top-news'>
                                        <div class="col-sm-2 news-date">
                                            June 2022
                                    </div>
                                    <div class="col">
                                        I published my work <a href="https://arxiv.org/pdf/2206.01661.pdf">Style-Content Disentanglement in Language-Image Pretraining Representations for Zero-Shot Sketch-to-Image Synthesis</a> on Arxiv. I found that CLIP embeddings exhibit a similar kind of compositionality as seen in word2vec embeddings, e.g, king - man + woman = queen, a hunch I had based on their sharing of contrastive training. 
                                        <br>
                            <br>
                                        My results demonstrate that this can be exploited to achieve an open-domain sketch-to-image model competitive with state-of-the-art instance-level models, while only depending on pretrained off-the-shelf models and a fraction of the data.
                                        
                                        <br>
                                        <br>
                
                                    </div>
                                </div>
                
                                    <div class='row vspace-top-news'>
                                        <div class="col-sm-2 news-date">
                                            May 2022
                                    </div>
                                    <div class="col">
                                        This month a new installation I made dubbed "Dream Machine" will be exhibited in <a href="https://www.deschoolamsterdam.nl/en/">De School</a> next to MIDIalogue. I'll soon publish a paper about the machine learning techniques I've used for it's sketch-to-image algorithm.
                                                                
                                        <br>
                            <br>
                                        The installation consists of a hacked 00's Xerox photocopier. In front of the Xerox machine is a drawing table with paper and markers, inviting visitors to get creative. The machine scans input drawings, generates an artistic image based on the content of the input, and prints the output.
                                        
                                        <br>
                                        <br>
                
                                    </div>
                                </div>
                
                
                                    <div class='row vspace-top-news'>
                                        <div class="col-sm-2 news-date">
                                            December 2021
                                    </div>
                                    <div class="col">
                                        My paper <a href="https://arxiv.org/abs/2111.08462">Towards Lightweight Controllable Audio Synthesis using Conditional Implicit Neural Representations</a> based on my work at AMLab is accepted at both <a href="https://neuripscreativityworkshop.github.io/2021/">Machine Learning for Creativity and Design</a> and <a href="https://neurips.cc/Conferences/2021/ScheduleMultitrack?event=21878">Deep Generative Models and Downstream Applications</a> at NeurIPS '21. I'm invited to give an oral presentation at the latter. 
                                        
                                        <br>
                                        <br>
                
                
                                        My work "MIDIalogue" is selected to be featured in the <a href="https://neuripscreativityworkshop.github.io/2021/#/gallery">Machine Learning for Creativity and Design</a> gallery.
                                    </div>
                                </div>
                                    
                                <br>
                                    <div class='row vspace-top-news'>
                                        <div class="col-sm-2 news-date">
                                            October 2021
                                    </div>
                                    <div class="col">
                                        My first installation, "MIDIalogue", will be exhibited at <a href="https://www.deschoolamsterdam.nl/en/">De School</a> for the coming editions of the "a Perception of Space and Time" series of events.
                                        
                                        <br>
                            <br>
                
                                        MIDIalogue functions as an interface between visitors and a transformer neural network trained for melody generation and continuation. The interface consists of immaterial, but visible and pluckable strings. Like a virtual harp. The machine listens to visitors' melodic input, produces musical reactions and imposes possible conversation directions accordingly, assuring a harmonious, ever-evolving 
                                        discourse. 
                
                                    </div>
                                </div>
                
                                <br>
                                    <div class='row vspace-top-news'>
                                        <div class="col-sm-2 news-date">
                                            September 2021
                                    </div>
                                    <div class="col">
                                       I graduated cum laude from the MSc Artificial Intelligence at the University of Amsterdam, with my thesis on implicit neural representations applied to Audio: <a href="docs/Jan_Thesis_Msc.pdf"> Representing Audio in a Distribution of Continuous Functions</a>.
                                       <br>
                                       <br>
                
                                       Also, I'm starting a new master: Art Science at the Royal Conservatoire and the Royal Academy of Art in The Hague. During my time here I'll be researching applications of machine learning in art.
                                    </div>
                                </div>
                                <br>
                                    <div class='row vspace-top-news'>
                                        <div class="col-sm-2 news-date">
                                            January 2021
                                    </div>
                                    <div class="col">
                                        I just started a research internship at the Amsterdam Machine Learning Lab (<a href="https://amlab.science.uva.nl/">AMLab</a>). I'll research applications of implicit neural representations in generative networks for audio synthesis, supervised by drs. Marco Federici and dr. Erik Bekkers.
                                    </div>
                                </div>